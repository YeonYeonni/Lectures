{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925dc1cd",
   "metadata": {},
   "source": [
    "## MLP 모델 설계 순서\n",
    "\n",
    "1. 필요한 모듈 import.\n",
    "2. 딥러닝 모델 활용 장비 확인.\n",
    "3. MNIST 데이터 다운로드하기 & Train, Test 분리하기.\n",
    "4. 데이터 확인하기. (1)\n",
    "5. 데이터 확인하기. (2)\n",
    "6. MLP 모델 설계하기.\n",
    "7. Optimizer, Objective Function 설정하기.\n",
    "8. 학습을 진행하면서 Train 데이터에 대한 모델 성능을 확인하는 함수 정의하기.\n",
    "9. 학습을 진행하면서 Train, Test set의 Loss 및 Test set Accuracy 확인하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7298c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np # (1)\n",
    "import matplotlib.pyplot as plt # (2)\n",
    "import torch # (3)\n",
    "import torch.nn as nn # (4)\n",
    "import torch.nn.functional as F # (5)\n",
    "from torchvision import transforms, datasets # (6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26481bd6",
   "metadata": {},
   "source": [
    "1. 선형 대수 관련 함수 모듈.\n",
    "2. 함수 실행 결과 산출물에 대한 수치 시각화.\n",
    "3. 파이토치의 기본 모듈.\n",
    "4. 파이토치 모듈 중 딥러닝 (인공 신경망 설계) 을 설계할 때 필요한 함수를 모아놓은 모듈.\n",
    "5. torch.nn 모듈 중에서도 자주 이용되는 함수를 F로 지정.\n",
    "6. 컴퓨터 비전 연구 분야에서 자주 이용하는 'torchvision' 모듈 내 'transforms', 'datasets' 함수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38faf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.10.0+cpu  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f658d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # (1)\n",
    "epochs = 10 # (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501793fd",
   "metadata": {},
   "source": [
    "1. batch_size : 학습할 때 필요한 데이터 개수의 단위.\n",
    "    - mini-batch 1개 단위에 대해 데이터가 32개로 구성.\n",
    "    - Iteration : 1개의 미니배치를 이용해 학습하는 횟수.\n",
    "    - Epoch : 전체 데이터를 이용해 학습을 진행한 횟수. 하이퍼파라미터.\n",
    "        - ex) 전체 데이터 1만개, 미니배치 1000개 -> 1epoch당 10회의 iteration이 발생.\n",
    "        \n",
    "2. epochs : 존재하고 있는 mini-batch를 전부 이용하는 횟수. 하이퍼파라미터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f611d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = '../data/MNIST',          # (1)\n",
    "                              train = True,\n",
    "                              download = True,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = '../data/MNIST',          # (2)\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, # (3)\n",
    "                                          batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,   # (4)\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6fc79",
   "metadata": {},
   "source": [
    "1. MNIST 데이터셋을 다운로드.\n",
    "2. MNIST 데이터셋을 다운로드.\n",
    "    - root : 데이터가 저장될 장소를 지정. (상위 디렉터리에 존재하는 data폴더 내 MNIST 폴더에 저장)\n",
    "    - train : True는 train_dataset으로 설정. False는 test_dataset으로 설정.\n",
    "    - download : 해당 데이터를 인터넷상에서 다운로드해 이용할 것인지.\n",
    "    - transform : MNIST는 이미지. 기본적인 전처리를 진행할 것인지. tensor 형태로 변경.\n",
    "        - 한 픽셀은 0 ~ 255 범위의 스칼라 값으로 구성. 이를 0 ~ 1 범위로 정규화.\n",
    "        - MLP 모델이 포함된 모델은 Input 데이터 값의 크기가 커질수록 과적합 or 불안정.\n",
    "            - 정규화 과정을 이용해 Input으로 이용하는 것이 좋음.\n",
    "            \n",
    "3. 다운로드한 데이터셋을 mini-batch 단위로 분리해 지정.\n",
    "4. 다운로드한 데이터셋을 mini-batch 단위로 분리해 지정.\n",
    "    - mini-batch 별로 데이터를 묶어 단위를 맞춤.\n",
    "    - 이미지 데이터를 batch size만큼 묶어 1개의 mini-batch를 구성.\n",
    "    - dataset : mini-batch 단위로 할당하고자 하는 데이터셋 지정.\n",
    "    - batch_size : mini-batch 1개 단위를 구성하는 데이터의 개수 지정.\n",
    "    - shuffle : 데이터의 순서를 섞고자 할 때 이용.\n",
    "        - 이미지 데이터의 특징을 학습하는 것이 아닌 Label 값을 학습하는 것을 방지하기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9908ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (x_train, y_train) in train_loader:\n",
    "    print('x_train:', x_train.size(), 'type:', x_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba0ee9",
   "metadata": {},
   "source": [
    "- x_train : 32개의 이미지가 1개의 mini-batch로 구성. 가로 28, 세로 28개의 픽셀로 구성. 채널은 1 (grayscale).\n",
    "- y_train : 32개의 이미지 데이터 각각에 label값이 1개씩 존재. (32개의 값을 가지고 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c210fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train[i, :, :, :].numpy().reshape(28 ,28), cmap = 'gray_r')\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea40f57",
   "metadata": {},
   "source": [
    "## 일반 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94698289",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP (Multi Layer Perceptron) 모델 설계하기 '''\n",
    "\n",
    "class Net(nn.Module):                      # (1)\n",
    "    \n",
    "    def __init__(self):                    # (2)\n",
    "        super(Net, self).__init__()        # (3)\n",
    "        self.fc1 = nn.Linear(28 * 28, 512) # (4)\n",
    "        self.fc2 = nn.Linear(512, 256)     # (5)\n",
    "        self.fc3 = nn.Linear(256, 10)      # (6)\n",
    "        \n",
    "    def forward(self, x):                  # (7)\n",
    "        x = x.view(-1, 28 * 28)            # (8)\n",
    "        x = self.fc1(x)                    # (9)\n",
    "        x = F.sigmoid(x)                   # (10)\n",
    "        x = self.fc2(x)                    # (11)\n",
    "        x = F.sigmoid(x)                   # (12)\n",
    "        x = self.fc3(x)                    # (13)\n",
    "        x = F.log_softmax(x, dim = 1)      # (14)\n",
    "        \n",
    "return x                                   # (15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573ee2e",
   "metadata": {},
   "source": [
    "1. nn.Module 클래스를 상속받는 Net 클래스를 정의.\n",
    "2. Net 클래스의 instance를 생성했을 때, 지니게 되는 성질을 정의.\n",
    "3. nn.Module 내의 메서드를 상속.\n",
    "4. 첫 번째 Fully Connected Layer를 정의.\n",
    "    - Fully Connected Layer : feed forward neural networks.\n",
    "    - 마지막 몇 개의 계층을 형성.\n",
    "    - Fully Connected Layer의 대한 input은 final Pooling 이나 Convolutional Layer.\n",
    "    - Pooling 이나 Convolution이 평면화되어 입력됨.\n",
    "        - MNIST 데이터를 Input으로 사용하기 위해 28, 28, 1 (가로, 세로, 채널) 크기의 노드 수를 Input으로 설정.\n",
    "        - 그 후 두 번째 FC Layer의 노드 수를 512개로 설정할 것이기 때문에 output의 노드 수는 512개로 설정.\n",
    "5. 두 번째 Fully Connected Layer를 정의.\n",
    "    - Input은 512 크기의 벡터 값을 사용하기 위해 512, 세 번째 FC Layer의 노드 수가 256개이기 때문에 Output은 256개.\n",
    "6. 세 번째 Fully Connected Layer를 정의.\n",
    "    - Input은 256 크기의 벡터 값을 사용하기 위해 256, output으로 사용하기 위한 노드 수를 10개로 설정.\n",
    "        - 0 ~ 9까지 10개의 클래스를 표현하기 위한 Label 값은 One-hot Encoding으로 표현.\n",
    "        - MLP 모델의 Output 값과 Loss를 계산하려면 이에 맞는 크기의 벡터를 계산해야 함. 따라서 Output의 노드 수를 10개로 정의.\n",
    "7. Net 클래스를 이용해 MLP 모델의 Forward Propagation을 정의.\n",
    "    - 즉, 설계한 MLP 모델에 데이터를 입력했을 때 Output을 계산하기까지의 과정을 나열.\n",
    "8. MNIST 이미지 데이터는 크기가 28 * 28인 2차원 데이터. MLP 모델은 1차원의 벡터 값을 입력으로 받아야 함.\n",
    "    - 이를 위해 View 메서드를 이용해 784 크기의 1차원 데이터로 변환. (Flatten)\n",
    "9. __init__() 메서드를 이용해 첫 번째 Fully Connected Layer에 1차원으로 펼친 이미지 데이터를 통과.\n",
    "10. torch.nn.functional 내 정의된 비선형 함수인 sigmoid()를 이용. 두 번째 FC Layer의 Input으로 계산.\n",
    "11. __init__() 메서드를 이용해 두 번째 Fully Connected Layer에 (10)에서 sigmoid() 함수를 이용해 계산된 결과값을 통과.\n",
    "12. torch.nn.functional 내 정의된 비선형 함수인 sigmoid()를 이용. 세 번째 FC Layer의 Input으로 계산.\n",
    "13. __init__() 메서드를 이용해 세 번째 Fully Connected Layer에 (11)에서 sigmoid() 함수를 이용해 계산된 결과값을 통과.\n",
    "14. torch.nn.functional 내 정의된 비선형 함수인 log.softmax()를 이용. 최종 output을 계산.\n",
    "    - 10가지 경우의 수 중 하나로 분류하는 일을 수행하기 때문에 softmax를 이용해 확률 값을 계산.\n",
    "    - log_softmax를 사용하는 이유 : 역전파 알고리즘을 사용시, Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있음.\n",
    "        - Log 함수 그래프의 기울기는 부드럽게 변화.\n",
    "15. 최종 계산된 x값을 output으로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03079ae",
   "metadata": {},
   "source": [
    "## MLP + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5faca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP (Multi Layer Perceptron) 모델 설계하기 '''\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536de011",
   "metadata": {},
   "source": [
    "- Dropout을 적용했을 때 일반화가 강해져 Test Accuracy가 높아지는 결과가 기대.\n",
    "    - 학습과 검증 데이터셋의 feature, 레이블의 분포 간 많은 차이가 있을 때 유효하게 작용.\n",
    "    - MNIST는 학습과 검증 데이터셋 간 많은 차이가 발생하지 않기 때문에 성능이 조금 하락할 수도 있음.\n",
    "        - Epoch을 늘려 추가로 학습을 진행하면 성능이 좋아짐.\n",
    "- Dropout은 보통 ReLU() 비선형 함수와 잘 어울림."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33054463",
   "metadata": {},
   "source": [
    "## Dropout + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP (Multi Layer Perceptron) 모델 설계하기 '''\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6115e8",
   "metadata": {},
   "source": [
    "- ReLU() 함수\n",
    "    - 0미만인 값은 0으로 계산, 양수 값은 그대로 반영.\n",
    "    - Gradient를 빠르게 계산, Back Propagation을 효과적으로 이용.\n",
    "- Sigmoid() 함수\n",
    "    - 0에서 멀어질 수록 Gradient 값이 0에 가까워 Back Propagation을 효과적으로 이용할 수 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730aa12",
   "metadata": {},
   "source": [
    "## Dropout + ReLU + Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f05701",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. MLP (Multi Layer Perceptron) 모델 설계하기 '''\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # (1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # (2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x) # (3)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x) # (4)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        \n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ca5fd",
   "metadata": {},
   "source": [
    "1. nn.BatchNorm()을 Class 내에서 이용하기 위해 self.batch_norm1 으로 정의. 512차원으로 설정.\n",
    "    - 첫 번째 Fully Connected Layer의 Output이 512 크기의 벡터 값이기 때문.\n",
    "2. nn.BatchNorm()을 Class 내에서 이용하기 위해 self.batch_norm2 으로 정의. 256차원으로 설정.\n",
    "    - 두 번째 Fully Connected Layer의 Output이 256크기의 벡터 값이기 때문.\n",
    "3. 첫 번째 Fully Connected Layer의 Output을 위에서 정의한 self.batch_norm1의 Input으로 이용.\n",
    "4. 두 번째 Fully Connected Layer의 Output을 위에서 정의한 self.batch_norm2의 Input으로 이용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743889dd",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3787533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(device)                                                   # (!)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # (2)\n",
    "criterion = nn.CrossEntropyLoss()                                          # (3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cd8c6",
   "metadata": {},
   "source": [
    "1. 위에서 정의한 MLP 모델을 기존에 선정한 device에 할당.\n",
    "2. Back Propagation을 이용해 파라미터를 업데이트할 때 이용하는 Optimizer를 정의. (SGD, eta=0.01, momentum=0.5)\n",
    "3. MLP 모델의 output과 계산될 Label 값은 Class를 표현하는 One-hot encoding 값.\n",
    "    - MLP 모델의 output값, One-hot encoding 값과의 loss는 CrossEntropy를 이용해 계산하기 위해\n",
    "    - criterion = 'nn.CrossEntropyLoss' 로 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3584216",
   "metadata": {},
   "source": [
    "## Dropout + ReLU + Batch Normalization + He Uniform Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init                # (1)\n",
    "def weight_init(m):                          # (2)\n",
    "    if isinstance(m, nn.Linear):             # (3)\n",
    "        init.kaiming_uniform_(m.weight.data) # (4)\n",
    "\n",
    "model = Net().to(device)\n",
    "model.apply(weight_init)                     # (5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b8608",
   "metadata": {},
   "source": [
    "1. init모듈 : 초깃값으로 설정되는 요소에 대한 모듈 (weight, bias 등)\n",
    "2. MLP 모델 내의 Weight를 초기화할 부분을 설정하기 위해 weight_init함수를 정의.\n",
    "3. MLP 모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정.\n",
    "4. nn.Linear에 해당하는 파라미터 값에 대해 he_initialization을 이용해 파라미터 값을 초기화.\n",
    "5. weight_init 함수를 Net() 클래스의 인스턴스인 model에 적용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491e258",
   "metadata": {},
   "source": [
    "## Dropout + ReLU + Batch Normalization + He Uniform Initialization + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init                # (1)\n",
    "def weight_init(m):                          # (2)\n",
    "    if isinstance(m, nn.Linear):             # (3)\n",
    "        init.kaiming_uniform_(m.weight.data) # (4)\n",
    "\n",
    "model = Net().to(device)\n",
    "model.apply(weight_init)                     # (5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2db21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()                                                # (1)\n",
    "    for batch_idx, (image, label) in enumerate (train_loader):  # (2)\n",
    "        image = image.to(device)                                 # (3)\n",
    "        label = label.to(device)                                 # (4)\n",
    "        optimizer.zero_grad()                                    # (5)\n",
    "        output = model(image)                                    # (6)\n",
    "        loss = criterion(output, label)                          # (7)\n",
    "        loss.backward()                                          # (8)\n",
    "        optimizer.step()                                         # (9)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(Epoch, batch_idx * len(image),\n",
    "                                                                                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                                                                                loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2dfd5e",
   "metadata": {},
   "source": [
    "1. 위에서 정의한 MLP 모델을 학습 상태로 지정.\n",
    "2. train_loader : 이미지 데이터, 레이블 데이터가 mini-batch 단위로 묶여 저장.\n",
    "    - 이를 순서대로 이용해 MLP 모형을 학습.\n",
    "3. 이미지 데이터를 기존 정의된 장비에 할당.\n",
    "4. 레이블 데이터를 기존 정의된 장비에 할당.\n",
    "5. 과거에 이용한 mini-batch 내의 이미지, 레이블 데이터를 바탕으로 계산된 loss의 gradient 값이 optimizer에 할당.\n",
    "    - optimizer의 gradient를 초기화.\n",
    "6. 이미지 데이터를 MLP 모델의 input으로 이용해 output을 계산.\n",
    "7. Output, 레이블 데이터를 CrossEntropy를 이용해 Loss를 계산.\n",
    "8. Loss를 계산한 결과를 바탕으로 Back Propagation을 통해 계산된 Gradient를 각 파라미터에 할당. (weight, bias)\n",
    "9. gradient 값을 이용해 파라미터 값을 업데이트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11a868b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의'''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()                                                              # (1)\n",
    "    test_loss = 0                                                             # (2)\n",
    "    correct = 0                                                               # (3)\n",
    "    \n",
    "    with torch.no_grad():                                                     # (4)\n",
    "        for image, label in test_loader:                                      # (5)\n",
    "            image = image.to(device)                                          # (6)\n",
    "            label = label.to(device)                                          # (7)\n",
    "            output = model(image)                                             # (8)\n",
    "            test_loss += criterion(output, label).item()                      # (9)\n",
    "            prediction = output.max(1, keepdim = True)[1]                    # (10)\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()  # (11)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)                                     # (12)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)                 # (13)\n",
    "    return test_loss, test_accuracy                                           # (14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b83ae",
   "metadata": {},
   "source": [
    "### 학습 과정, 학습 완료 상태에서 성능을 평가하기 위한 evaluation 함수를 정의.\n",
    "1. MLP 모델을 학습 상태 -> 평가 상태로 지정.\n",
    "2. test_loader 내의 데이터를 이용해 Loss 값을 계산하기 위해 test_loss를 0으로 임시 설정.\n",
    "3. 모델이 올바른 Class로 분류한 경우를 세기 위해 correct를 0으로 임시 설정.\n",
    "4. 평가 단계에서 Gradient를 통해 업데이트되는 현상을 방지하기 위해 torch.no_grad() 메서드를 이용해 흐름을 억제.\n",
    "5. test_loader 데이터도 mini-batch 단위로 저장되어 있음. mini-batch 내에 있는 이미지, 레이블 데이터를 차례대로 접근.\n",
    "6. 이미지 데이터로 모델을 검증하기 위해 기존에 정의한 장비에 할당.\n",
    "7. 레이블 데이터를 기존에 정의한 장비에 할당.\n",
    "8. 이미지 데이터를 Input으로 Output을 계산.\n",
    "9. output과 레이블 데이터를 CrossEntropy를 이용해 Loss값을 계산한 결과값을 'test_loss'에 더해 업데이트.\n",
    "10. MLP 모델의 Output값은 크기가 10인 벡터 값. \n",
    "    - 계산된 벡터값 내 가장 큰 값인 위치에 대해 해당 위치에 대응하는 클래스로 예측했다고 판단.\n",
    "11. MLP 모델이 최종으로 예측한 클래스 값과 실제 레이블이 의미하는 클래스가 맞으면 correct에 더해 올바른 예측 횟수를 저장.\n",
    "12. 현재까지 계산된 test_loss 값 = test_loader 내의 mini-batch 개수만큼 나눠 평균 loss 값으로 계산.\n",
    "13. test_loader 데이터 중 얼마나 맞췄는지 정확도를 계산.\n",
    "14. test_loss, test_accuracy를 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e0e320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.347106\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.333780\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.319917\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.305351\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.326942\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.310081\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.326447\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.281296\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.250386\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.276880\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0698, \tTest Accuracy: 37.92 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.271647\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.199564\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.162466\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.187071\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 1.932872\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 1.893052\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 1.614942\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 1.453866\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 1.375365\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 1.259325\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0395, \tTest Accuracy: 62.42 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 1.368065\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 1.392086\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 1.043114\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.105586\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.986954\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.122661\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.889307\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.741611\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.661517\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.899634\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0236, \tTest Accuracy: 78.28 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.799649\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.527990\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.737318\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.611594\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.884956\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.494849\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.451604\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.628272\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.606606\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.489451\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0174, \tTest Accuracy: 83.88 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.418029\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.486251\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.360959\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.419325\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.643173\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.642063\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.333559\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.455491\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.785895\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.590097\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0143, \tTest Accuracy: 86.64 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.543907\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.477121\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.282460\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.336385\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.316596\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.573582\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.525577\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.335715\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.349857\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.451218\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0128, \tTest Accuracy: 87.96 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.790787\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.287266\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.301827\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.438557\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.437227\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.805833\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.494078\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.573067\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.566101\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.417376\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0118, \tTest Accuracy: 89.09 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.155974\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.150856\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.308649\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.695224\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.327975\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.304196\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.336441\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.297247\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.473716\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.531299\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0113, \tTest Accuracy: 89.54 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.624764\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.310156\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.515030\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.470907\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.672383\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.353380\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.403435\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.541840\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.654042\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.859685\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0108, \tTest Accuracy: 90.16 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.267974\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.230260\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.379603\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.231348\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.161614\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.385716\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.284891\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.159481\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.198292\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.112461\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0105, \tTest Accuracy: 90.41 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''10. MLP 학습을 실행하면서 Train, Test set의 Loss 및 Test set Accuracy를 확인하기'''\n",
    "for Epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)   # (1)\n",
    "    test_loss, test_accuracy = evaluate (model, test_loader)    # (2)\n",
    "    print('\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n'.format(Epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73956caa",
   "metadata": {},
   "source": [
    "1. 정의한 train 함수를 실행.\n",
    "    - model : MLP\n",
    "    - train_loader : 학습 데이터\n",
    "    - optimizer : SGD\n",
    "    - log_interval : mini-batch의 index를 이용해 과정을 모니터링.\n",
    "2. 각 epoch 별로 Loss값과 accuracy값을 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bac631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
