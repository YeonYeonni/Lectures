{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71595e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b266fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "datasets.CIFAR10('./data/CIFAR_10/',\n",
    "                train = True,\n",
    "                download = True,\n",
    "                 # Compose를 이용해 전처리를 진행.\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    # 평균, 표준편차를 이용해 normalize\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                        (0.5, 0.5, 0.5))])), batch_size = 64, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "datasets.CIFAR10('./data/CIFAR_10/',\n",
    "                train = False,\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                        (0.5, 0.5, 0.5))])), batch_size = 64) # test는 shuffle이 필요 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdcfc782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cpu\n",
      "MODEL:  CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",
      "  (conv1_bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1)\n",
    "        # self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, padding = 1, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "\n",
    "        # self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        # self.fc1 = nn.Linear(8 * 8 * 16, 64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.fc1 = nn.Linear(2 * 2 * 64, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        \n",
    "        # bn\n",
    "        self.conv1_bn = nn.BatchNorm2d(8) # output chanel을 인자로.\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3_bn = nn.BatchNorm2d(32)\n",
    "        self.conv4_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout_p = 0.2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 32 * 32 * 3 -> 32 * 32 * 8 (kernel_size, padding, stride에 영향)\n",
    "        x = self.conv1_bn(x) # bn 적용. 위치는 activation 전, 후. (명확히 밝혀지지 않음)\n",
    "        '''\n",
    "        activation은 비선형 함수이므로 앞의 내용과 많은 차이를 보임.\n",
    "        보존되지 않은 데이터에 bn을 적용하는 것은 적합하지 않다는 의견.\n",
    "        '''\n",
    "        x = F.tanh(x)     # 32 * 32 * 8\n",
    "        x = self.pool(x)  # 16 * 16 * 8 (pooling시 kernel_size, stride가 2이기 때문에 절반)\n",
    "        \n",
    "        x = self.conv2(x) # 16 * 16 * 8 -> 16 * 16 * 16\n",
    "        x = self.conv2_bn(x)\n",
    "        x = F.tanh(x)     # 16 * 16 * 16\n",
    "        x = self.pool(x)  # 8 * 8 * 16\n",
    "        \n",
    "        x = self.conv3(x) # 8 * 8 * 16 -> 8 * 8 * 32\n",
    "        x = self.conv3_bn(x)\n",
    "        x = F.tanh(x)     # 8 * 8 * 32\n",
    "        x = self.pool(x)  # 4 * 4 * 32\n",
    "        \n",
    "        x = self.conv4(x) # 4 * 4 * 32 -> 4 * 4 * 64\n",
    "        x = self.conv4_bn(x)\n",
    "        x = F.tanh(x)     # 4 * 4 * 64\n",
    "        x = self.pool(x)  # 2 * 2 * 64. line 14, 37의 input.\n",
    "        \n",
    "        x = x.view(-1, 2 * 2 * 64)\n",
    "        x = self.fc1(x)\n",
    "        x = F.self.dropout(x, p = self.dropout_p) # dropout 역시 activation 전, 후.\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.self.dropout(x, p = self.dropout_p)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x\n",
    "    \n",
    "# Weight Initialize\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    '''\n",
    "    Ref: https://pytorch.org/docs/stable/nn.init.html\n",
    "    \n",
    "    균등분포) init.uniform_(tensorm, a = 0.0, b = 1.0) (a: Lower bound, b: Upper bound)\n",
    "    정규분포) init.normal_(tensor, mean = 0.0, std = 1.0)\n",
    "    init.xavier_uniform_(tensor, gain = 1.0)\n",
    "    init.xavier_normal_(tensor, gain = 1.0)\n",
    "    init.kaiming_uniform_(tensor, a = 0, mode = 'fan_in', nonlinearity = 'leakey_relu')\n",
    "    init.kaiming_normal_(tensor, a = 0, mode = 'fan_in', nonlinearity = 'leakey_relu')\n",
    "    '''\n",
    "    \n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_uniform_(m.weight.data) # xavier_normal 분포에서 초기화\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "            \n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean = 1, std = 0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "        \n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    \n",
    "    '''\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.uniform_(m.weight.data) # xavier_normal 분포에서 초기화\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "            \n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean = 1, std = 0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "        \n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.uniform_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    '''\n",
    "    \n",
    "model = CNN().to(DEVICE)\n",
    "model.apply(weight_init) # weight_init 적용\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "print('DEVICE: ', DEVICE)\n",
    "print('MODEL: ', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77068f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): # train_loader 내 image와 label에 대해.\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83fd177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction = 'sum').item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c45ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000(0%)]\tTrain Loss: 4.650102\n",
      "Train Epoch: 1 [6400/50000(13%)]\tTrain Loss: 2.310274\n",
      "Train Epoch: 1 [12800/50000(26%)]\tTrain Loss: 2.074276\n",
      "Train Epoch: 1 [19200/50000(38%)]\tTrain Loss: 1.914835\n",
      "Train Epoch: 1 [25600/50000(51%)]\tTrain Loss: 1.667314\n",
      "Train Epoch: 1 [32000/50000(64%)]\tTrain Loss: 1.516175\n",
      "Train Epoch: 1 [38400/50000(77%)]\tTrain Loss: 1.613272\n",
      "Train Epoch: 1 [44800/50000(90%)]\tTrain Loss: 1.556158\n",
      "[1] Test Loss: 1.4879, accuracy: 46.00%\n",
      "\n",
      "Train Epoch: 2 [0/50000(0%)]\tTrain Loss: 1.558417\n",
      "Train Epoch: 2 [6400/50000(13%)]\tTrain Loss: 1.366343\n",
      "Train Epoch: 2 [12800/50000(26%)]\tTrain Loss: 1.327870\n",
      "Train Epoch: 2 [19200/50000(38%)]\tTrain Loss: 1.182071\n",
      "Train Epoch: 2 [25600/50000(51%)]\tTrain Loss: 1.663728\n",
      "Train Epoch: 2 [32000/50000(64%)]\tTrain Loss: 1.182510\n",
      "Train Epoch: 2 [38400/50000(77%)]\tTrain Loss: 1.346214\n",
      "Train Epoch: 2 [44800/50000(90%)]\tTrain Loss: 1.190817\n",
      "[2] Test Loss: 1.2957, accuracy: 52.97%\n",
      "\n",
      "Train Epoch: 3 [0/50000(0%)]\tTrain Loss: 1.174314\n",
      "Train Epoch: 3 [6400/50000(13%)]\tTrain Loss: 1.408046\n",
      "Train Epoch: 3 [12800/50000(26%)]\tTrain Loss: 1.412963\n",
      "Train Epoch: 3 [19200/50000(38%)]\tTrain Loss: 1.036168\n",
      "Train Epoch: 3 [25600/50000(51%)]\tTrain Loss: 1.584741\n",
      "Train Epoch: 3 [32000/50000(64%)]\tTrain Loss: 1.168567\n",
      "Train Epoch: 3 [38400/50000(77%)]\tTrain Loss: 1.194689\n",
      "Train Epoch: 3 [44800/50000(90%)]\tTrain Loss: 1.173691\n",
      "[3] Test Loss: 1.2247, accuracy: 56.31%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print('[{}] Test Loss: {:.4f}, accuracy: {:.2f}%\\n'.format(epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6341bf76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN's number of Parameters:  43386\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN's number of Parameters: \", sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5534b2",
   "metadata": {},
   "source": [
    "### 튜닝 과정 (아키텍처, 모델 구조 부분)\n",
    "\n",
    "- 1. Activation tuning. (Parameter 수는 차이가 없음) (Parameter수가 model architecture의 크기)\n",
    "    - sigmoid\n",
    "    - tanh\n",
    "    - leaky_relu\n",
    "    \n",
    "- 2. Convolution 층 증가. (각각의 층에서 파라미터 수의 변화를 계산할 수 있어야 함)\n",
    "    - Parameter 수가 줄어듦\n",
    "        - conv -> fc로 넘어갈 때 변화가 크게 일어남.\n",
    "        - kernel_size와 stride가 2 => pooling size가 굉장히 큼.\n",
    "            - feature map의 크기가 절반으로 계속 줄어들기 때문.\n",
    "\n",
    "- 3. Pooling 교체.\n",
    "    - MaxPooling\n",
    "        - 최대값을 제외한 나머지 값은 고려되지 않음.\n",
    "        - task) image를 input으로 받아 label로 분류. image에서 어느 특정 부분이 중요한지 focusing 하는 것이 중요.\n",
    "    - AvgPooling\n",
    "        - 모든 요소의 값을 고려.\n",
    "        - task) pixel 하나하나가 중요할 경우. detecting 등.\n",
    "        \n",
    "- 4. stride\n",
    "    - weight가 이동하는 간격.\n",
    "    - stride가 1에서 2로 변경되면, 32 * 32 * 3 -> 16 * 16 * 8\n",
    "    - parameter 수가 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4383d",
   "metadata": {},
   "source": [
    "### 튜닝 과정 (아키텍처 외적인 부분)\n",
    "1. Weight initialization.\n",
    "\n",
    "2. Batch Normalization.\n",
    "    - Layer가 깊어짐에 따라 Gradient가 전달이 안될 때 사용.\n",
    "        - ex) residual connection 방법\n",
    "    - 깊이가 깊거나 parameter이 많을 때, train은 잘 맞추지만 test를 잘 맞추지 못할 경우,\n",
    "        - 안정적 수렴.\n",
    "        - 과적합 방지용.\n",
    "            - 성능이 잘 나오는 모델에 추가로 적용하는 것을 추천. (90% 이상)\n",
    "            \n",
    "3. Dropout\n",
    "    - 학습 때마다 노드의 Weight를 랜덤으로 초기화.\n",
    "        - 앙상블 효과가 있음.\n",
    "        - 성능이 잘 나오는 모델에 추가로 적용하는 것을 추천.\n",
    "        \n",
    "4. Batch Size\n",
    "    - 64개의 데이터를 넣은 결과 64개의 loss를 평균 내서 역전파 진행.\n",
    "    - Batch Size가 커지면 iteration이 작아짐.\n",
    "        - ex) Batch Size = 1이면 데이터 한 개 마다 역전파를 진행. 학습을 많이 진행한다는 의미. (느림)\n",
    "    - Batch Normalization과 연관.\n",
    "    - 2의 배수 승으로 초기화. 32, 64, 128, 256, 512 등\n",
    "    \n",
    "5. Optimizer\n",
    "    - optim. tab으로 확인.\n",
    "    - lr 등 하이퍼 파라미터도 조정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52233d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
